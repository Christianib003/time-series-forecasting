{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /Users/testsolutions/Documents/school/year3/term2/time-series-forecasting/tsf-repo\n"
     ]
    }
   ],
   "source": [
    "# Make project root importable (same Cell 0 you’ve been using)\n",
    "import sys, importlib\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "SRC = ROOT / \"src\"\n",
    "\n",
    "# Ensure src package structure exists (idempotent)\n",
    "SRC.mkdir(parents=True, exist_ok=True)\n",
    "(SRC / \"__init__.py\").touch(exist_ok=True)\n",
    "(SRC / \"models\").mkdir(parents=True, exist_ok=True)\n",
    "(SRC / \"models\" / \"__init__.py\").touch(exist_ok=True)\n",
    "\n",
    "# Ensure src/utils.py exists\n",
    "(SRC / \"utils.py\").touch(exist_ok=True)\n",
    "(SRC / \"config.py\").touch(exist_ok=True)\n",
    "(SRC / \"cv.py\").touch(exist_ok=True)\n",
    "(SRC / \"window.py\").touch(exist_ok=True)\n",
    "(SRC / \"scaling.py\").touch(exist_ok=True)\n",
    "(SRC / \"metrics.py\").touch(exist_ok=True)\n",
    "(SRC / \"models\" / \"lstm.py\").touch(exist_ok=True)\n",
    "\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "importlib.invalidate_caches()\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from src.utils import seed_all, get_logger\n",
    "from src.config import DATA_INTERIM, DATA_RAW, OUTPUTS, MODELS_DIR, SUBMISSIONS_DIR\n",
    "from src.features import add_target_lags\n",
    "from src.inference import final_fit_windows, rollout_forecast_test\n",
    "from src.models.lstm import build_lstm\n",
    "\n",
    "seed_all(42)\n",
    "log = get_logger(\"FINALFIT\")\n",
    "OUTPUTS.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SUBMISSIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === Choose the BEST LSTM hyperparams from your experiments ===\n",
    "LOOKBACK   = 168        # update if a better run used 72 or 336\n",
    "UNITS      = 64         # update if 128 was better\n",
    "N_LAYERS   = 2\n",
    "DROPOUT    = 0.2        # update if 0.1 was better\n",
    "REC_DROPOUT= 0.0\n",
    "LR         = 1e-3\n",
    "CLIPNORM   = 1.0\n",
    "BATCH      = 64\n",
    "EPOCHS     = 60\n",
    "PATIENCE   = 10\n",
    "\n",
    "TARGET = \"pm2_5\"\n",
    "LAGS   = (1, 24, 168)\n",
    "MAX_LAG = max(LAGS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features: 20\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_INTERIM / \"clean.csv\", parse_dates=[\"datetime\"])\n",
    "train_df = df.query(\"split=='train'\").reset_index(drop=True)\n",
    "test_df  = df.query(\"split=='test'\").reset_index(drop=True)\n",
    "\n",
    "# Add target lags to BOTH; test lags will be populated during rollout\n",
    "train_df = add_target_lags(train_df, target=TARGET, lags=LAGS)\n",
    "test_df  = add_target_lags(test_df,  target=TARGET, lags=LAGS)\n",
    "\n",
    "# Feature list (same contract as training notebooks; exclude noise 'No')\n",
    "EXCLUDE = {\"datetime\",\"split\",\"pm2.5\",\"pm2_5\",\"No\"}\n",
    "feature_cols = [c for c in train_df.columns if c not in EXCLUDE]\n",
    "print(\"n_features:\", len(feature_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 58ms/step - loss: 0.9810 - val_loss: 0.2474 - learning_rate: 0.0010\n",
      "Epoch 2/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 55ms/step - loss: 0.3545 - val_loss: 0.1994 - learning_rate: 0.0010\n",
      "Epoch 3/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 53ms/step - loss: 0.3203 - val_loss: 0.1792 - learning_rate: 0.0010\n",
      "Epoch 4/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 56ms/step - loss: 0.3056 - val_loss: 0.1773 - learning_rate: 0.0010\n",
      "Epoch 5/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 54ms/step - loss: 0.2912 - val_loss: 0.1655 - learning_rate: 0.0010\n",
      "Epoch 6/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 62ms/step - loss: 0.2700 - val_loss: 0.1685 - learning_rate: 0.0010\n",
      "Epoch 7/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 65ms/step - loss: 0.2583 - val_loss: 0.1607 - learning_rate: 0.0010\n",
      "Epoch 8/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 78ms/step - loss: 0.2506 - val_loss: 0.1570 - learning_rate: 0.0010\n",
      "Epoch 9/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 74ms/step - loss: 0.2433 - val_loss: 0.1601 - learning_rate: 0.0010\n",
      "Epoch 10/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 81ms/step - loss: 0.2370 - val_loss: 0.1499 - learning_rate: 0.0010\n",
      "Epoch 11/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 80ms/step - loss: 0.2272 - val_loss: 0.1491 - learning_rate: 0.0010\n",
      "Epoch 12/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 73ms/step - loss: 0.2226 - val_loss: 0.1733 - learning_rate: 0.0010\n",
      "Epoch 13/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 73ms/step - loss: 0.2166 - val_loss: 0.1577 - learning_rate: 0.0010\n",
      "Epoch 14/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 75ms/step - loss: 0.2109 - val_loss: 0.1433 - learning_rate: 0.0010\n",
      "Epoch 15/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 67ms/step - loss: 0.2087 - val_loss: 0.1459 - learning_rate: 0.0010\n",
      "Epoch 16/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 65ms/step - loss: 0.2127 - val_loss: 0.1419 - learning_rate: 0.0010\n",
      "Epoch 17/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 65ms/step - loss: 0.2046 - val_loss: 0.1467 - learning_rate: 0.0010\n",
      "Epoch 18/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 68ms/step - loss: 0.2020 - val_loss: 0.1486 - learning_rate: 0.0010\n",
      "Epoch 19/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 69ms/step - loss: 0.2009 - val_loss: 0.1465 - learning_rate: 0.0010\n",
      "Epoch 20/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 70ms/step - loss: 0.2005 - val_loss: 0.1567 - learning_rate: 0.0010\n",
      "Epoch 21/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 70ms/step - loss: 0.1988 - val_loss: 0.1444 - learning_rate: 0.0010\n",
      "Epoch 22/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 67ms/step - loss: 0.1902 - val_loss: 0.1560 - learning_rate: 5.0000e-04\n",
      "Epoch 23/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 69ms/step - loss: 0.1864 - val_loss: 0.1564 - learning_rate: 5.0000e-04\n",
      "Epoch 24/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 74ms/step - loss: 0.1888 - val_loss: 0.1625 - learning_rate: 5.0000e-04\n",
      "Epoch 25/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 70ms/step - loss: 0.1818 - val_loss: 0.1511 - learning_rate: 5.0000e-04\n",
      "Epoch 26/60\n",
      "\u001b[1m400/400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 76ms/step - loss: 0.1804 - val_loss: 0.1623 - learning_rate: 5.0000e-04\n",
      "Saved: /Users/testsolutions/Documents/school/year3/term2/time-series-forecasting/tsf-repo/outputs/models/lstm_FULL_lb168_u64_L2_final.keras\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Windows + scaler\n",
    "Xw_tr, yw_tr, scaler = final_fit_windows(\n",
    "    df_train=train_df,\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET,\n",
    "    lookback=LOOKBACK,\n",
    "    max_lag=MAX_LAG,\n",
    ")\n",
    "\n",
    "# Build model\n",
    "model = build_lstm(\n",
    "    input_len=LOOKBACK, n_features=Xw_tr.shape[2],\n",
    "    units=UNITS, n_layers=N_LAYERS,\n",
    "    dropout=DROPOUT, recurrent_dropout=REC_DROPOUT,\n",
    "    clipnorm=CLIPNORM, lr=LR\n",
    ")\n",
    "\n",
    "ckpt_path = MODELS_DIR / f\"lstm_FULL_lb{LOOKBACK}_u{UNITS}_L{N_LAYERS}.keras\"\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=PATIENCE, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=max(3, PATIENCE//2), min_lr=1e-5),\n",
    "    ModelCheckpoint(filepath=str(ckpt_path), monitor=\"val_loss\", save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    Xw_tr, yw_tr,\n",
    "    validation_split=0.1,   # small internal split for callbacks\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Save final model (best weights are already restored)\n",
    "final_model_path = MODELS_DIR / f\"lstm_FULL_lb{LOOKBACK}_u{UNITS}_L{N_LAYERS}_final.keras\"\n",
    "model.save(final_model_path)\n",
    "print(\"Saved:\", final_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test preds shape: (13148,)\n",
      "First 5 preds: [18.99878  19.658453 21.492937 22.690647 23.372854]\n"
     ]
    }
   ],
   "source": [
    "# Rollout predictions across the entire test horizon\n",
    "y_test_pred = rollout_forecast_test(\n",
    "    model=model,\n",
    "    df_train=train_df,\n",
    "    df_test=test_df,\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=TARGET,\n",
    "    lags=LAGS,\n",
    "    lookback=LOOKBACK,\n",
    "    scaler=scaler,\n",
    ")\n",
    "\n",
    "# Non-negativity & (optional) light clipping against absurd spikes\n",
    "y_test_pred = np.clip(y_test_pred, 0, None)\n",
    "\n",
    "print(\"Test preds shape:\", y_test_pred.shape)\n",
    "print(\"First 5 preds:\", y_test_pred[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw preds: /Users/testsolutions/Documents/school/year3/term2/time-series-forecasting/tsf-repo/outputs/predictions/preds_lstm_lb168_u64_L2_20250921T182112Z.npy\n",
      "Loaded sample submission: /Users/testsolutions/Documents/school/year3/term2/time-series-forecasting/tsf-repo/data/raw/sample_submission.csv\n",
      "Wrote NEW aligned submission: /Users/testsolutions/Documents/school/year3/term2/time-series-forecasting/tsf-repo/outputs/submissions/submission_lstm_lb168_u64_L2_aligned_20250921T182112Z.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row ID</th>\n",
       "      <th>pm2.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-07-02 4:00:00</td>\n",
       "      <td>18.998779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-07-02 5:00:00</td>\n",
       "      <td>19.658453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-07-02 6:00:00</td>\n",
       "      <td>21.492937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-07-02 7:00:00</td>\n",
       "      <td>22.690647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-07-02 8:00:00</td>\n",
       "      <td>23.372854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               row ID      pm2.5\n",
       "0  2013-07-02 4:00:00  18.998779\n",
       "1  2013-07-02 5:00:00  19.658453\n",
       "2  2013-07-02 6:00:00  21.492937\n",
       "3  2013-07-02 7:00:00  22.690647\n",
       "4  2013-07-02 8:00:00  23.372854"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 0) Ensure we have y_test_pred in memory ---\n",
    "assert 'y_test_pred' in globals(), \"y_test_pred not found. Run the rollout cell first.\"\n",
    "\n",
    "# --- 1) Save predictions to a *new* file (for provenance) ---\n",
    "PRED_DIR = OUTPUTS / \"predictions\"\n",
    "PRED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "pred_path = PRED_DIR / f\"preds_lstm_lb{LOOKBACK}_u{UNITS}_L{N_LAYERS}_{ts}.npy\"\n",
    "np.save(pred_path, y_test_pred)\n",
    "print(\"Saved raw preds:\", pred_path)\n",
    "\n",
    "# --- 2) Load the official sample (Path-safe) ---\n",
    "from pathlib import Path\n",
    "candidates = [\n",
    "    (DATA_RAW / \"sample_submission.csv\") if hasattr(DATA_RAW, \"joinpath\") else Path(DATA_RAW) / \"sample_submission.csv\",\n",
    "    Path(\"sample_submission.csv\"),\n",
    "    Path.cwd().parent / \"sample_submission.csv\",\n",
    "]\n",
    "sample_df = None\n",
    "for p in candidates:\n",
    "    if p.exists():\n",
    "        sample_df = pd.read_csv(p)\n",
    "        print(\"Loaded sample submission:\", p)\n",
    "        break\n",
    "assert sample_df is not None, \"Could not find sample_submission.csv\"\n",
    "\n",
    "# --- 3) Align predictions to sample order (No-preferred; else time key) ---\n",
    "sub = sample_df.copy()\n",
    "target_col_name = \"pm2.5\" if \"pm2.5\" in sub.columns else (\"pm2_5\" if \"pm2_5\" in sub.columns else sub.columns[-1])\n",
    "\n",
    "if (\"No\" in test_df.columns) and (\"No\" in sub.columns):\n",
    "    pred_map = pd.Series(y_test_pred, index=test_df[\"No\"]).to_dict()\n",
    "    sub[target_col_name] = sub[\"No\"].map(pred_map)\n",
    "else:\n",
    "    # Find a time-like key in sample\n",
    "    sample_time_col = None\n",
    "    for cand in [\"row ID\", \"row_id\", \"datetime\", \"timestamp\", \"date_time\"]:\n",
    "        if cand in sub.columns:\n",
    "            sample_time_col = cand\n",
    "            break\n",
    "    assert sample_time_col is not None, \"No alignable time-like key in sample.\"\n",
    "    pred_map = pd.Series(y_test_pred, index=pd.to_datetime(test_df[\"datetime\"])).to_dict()\n",
    "    sub[target_col_name] = [pred_map.get(t, np.nan) for t in pd.to_datetime(sub[sample_time_col])]\n",
    "\n",
    "# --- 4) Sanity checks ---\n",
    "assert sub[target_col_name].isna().sum() == 0, \"Alignment produced NaNs.\"\n",
    "assert (sub[target_col_name] >= 0).all(), \"Negative predictions found.\"\n",
    "\n",
    "# --- 5) Write a NEW submission with a unique name (no overwrite) ---\n",
    "SUBMISSIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "sub_path = SUBMISSIONS_DIR / f\"submission_lstm_lb{LOOKBACK}_u{UNITS}_L{N_LAYERS}_aligned_{ts}.csv\"\n",
    "sub.to_csv(sub_path, index=False)\n",
    "print(\"Wrote NEW aligned submission:\", sub_path)\n",
    "display(sub.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
